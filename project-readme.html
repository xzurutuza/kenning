

<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="">
    
    
      
        <title>Kenning - Kenning</title>
      
    
    
      
        
        
      
      

    
    
    
      
        
        
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
        <link rel="stylesheet" type="text/css" href="_static/sphinx_immaterial_theme.fa3fbbd75cfdd7cef.min.css" />
        <link rel="stylesheet" type="text/css" href="_static/css/bokeh.css" />
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="deep-orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="Kenning" class="md-header__button md-logo" aria-label="Kenning" data-md-component="logo">
      <img src="_static/white.svg" alt="logo">
    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Kenning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Kenning
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <div class="md-header__button">
        <a href="kenning.pdf" title="PDF - kenning.pdf">
          <div class="md-icon">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V160H256c-17.7 0-32-14.3-32-32V0H64zm192 0v128h128L256 0zM64 224h24c30.9 0 56 25.1 56 56s-25.1 56-56 56h-8v32c0 8.8-7.2 16-16 16s-16-7.2-16-16V240c0-8.8 7.2-16 16-16zm24 80c13.3 0 24-10.7 24-24s-10.7-24-24-24h-8v48h8zm72-64c0-8.8 7.2-16 16-16h24c26.5 0 48 21.5 48 48v64c0 26.5-21.5 48-48 48h-24c-8.8 0-16-7.2-16-16V240zm32 112h8c8.8 0 16-7.2 16-16v-64c0-8.8-7.2-16-16-16h-8v96zm96-128h48c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v32h32c8.8 0 16 7.2 16 16s-7.2 16-16 16h-32v48c0 8.8-7.2 16-16 16s-16-7.2-16-16V240c0-8.8 7.2-16 16-16z"/></svg>
          </div>
        </a>
      </div>
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/antmicro/kenning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    antmicro/kenning
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="Kenning" class="md-nav__button md-logo" aria-label="Kenning" data-md-component="logo">
      <img src="_static/white.svg" alt="logo">
    </a>
    Kenning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/antmicro/kenning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    antmicro/kenning
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="introduction.html" class="md-nav__link">
        <span class="md-ellipsis">Introduction</span>
      </a>
    </li>
  

    
      
      
      

  
  
    
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          <span class="md-ellipsis">Kenning</span>
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="#" class="md-nav__link md-nav__link--active">
        <span class="md-ellipsis">Kenning</span>
      </a>
      
        

<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#kenning-installation" class="md-nav__link">
    <span class="md-ellipsis">Kenning installation</span>
  </a>
  
    <nav class="md-nav" aria-label="Kenning installation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#module-installation-with-pip" class="md-nav__link">
    <span class="md-ellipsis">Module installation with pip</span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#working-directly-with-the-repository" class="md-nav__link">
    <span class="md-ellipsis">Working directly with the repository</span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kenning-structure" class="md-nav__link">
    <span class="md-ellipsis">Kenning structure</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kenning-usage" class="md-nav__link">
    <span class="md-ellipsis">Kenning usage</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-use-case-of-kenning" class="md-nav__link">
    <span class="md-ellipsis">Example use case of Kenning</span>
  </a>
  
    <nav class="md-nav" aria-label="Example use case of Kenning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#benchmarking-a-model-using-a-native-framework" class="md-nav__link">
    <span class="md-ellipsis">Benchmarking a model using a native framework</span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizing-a-model-using-tensorflow-lite" class="md-nav__link">
    <span class="md-ellipsis">Optimizing a model using Tensor<wbr>Flow Lite</span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantizing-a-model-using-tensorflow-lite" class="md-nav__link">
    <span class="md-ellipsis">Quantizing a model using Tensor<wbr>Flow Lite</span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speeding-up-inference-with-apache-tvm" class="md-nav__link">
    <span class="md-ellipsis">Speeding up inference with Apache TVM</span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#automated-model-comparison" class="md-nav__link">
    <span class="md-ellipsis">Automated model comparison</span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-kenning-as-a-library-in-python-scripts" class="md-nav__link">
    <span class="md-ellipsis">Using Kenning as a library in Python scripts</span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#adding-new-implementations" class="md-nav__link">
    <span class="md-ellipsis">Adding new implementations</span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="dl-deployment-stack.html" class="md-nav__link">
        <span class="md-ellipsis">Deep Learning deployment stack</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="json-scenarios.html" class="md-nav__link">
        <span class="md-ellipsis">Defining optimization pipelines in Kenning</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="cmd-usage.html" class="md-nav__link">
        <span class="md-ellipsis">Using Kenning via command line arguments</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="kenning-measurements.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning measurements</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="onnx-conversion-support.html" class="md-nav__link">
        <span class="md-ellipsis">ONNX support in deep learning frameworks</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="sample-report.html" class="md-nav__link">
        <span class="md-ellipsis">Sample autogenerated report</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="kenning-flow.html" class="md-nav__link">
        <span class="md-ellipsis">Creating applications with Kenning</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="kenning-development.html" class="md-nav__link">
        <span class="md-ellipsis">Developing Kenning blocks</span>
      </a>
    </li>
  

    
      
      
      

  
  
  
  
    <li class="md-nav__item">
      <a href="kenning-api.html" class="md-nav__link">
        <span class="md-ellipsis">Kenning API</span>
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset" role="main">
                  


  <a href="https://github.com/antmicro/kenning/blob/main/docs/source/project-readme.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>

<h1 id="kenning">Kenning<a class="headerlink" href="#kenning" title="Permalink to this heading">¶</a></h1>
<p>Copyright (c) 2020-2023 <a class="reference external" href="https://www.antmicro.com">Antmicro</a></p>
<p><img alt="Kenning" src="_images/kenninglogo.png" /></p>
<p>Kenning is a framework for creating deployment flows and runtimes for Deep Neural Network applications on various target hardware.</p>
<p><a class="reference external" href="https://antmicro.github.io/kenning/">Kenning documentation</a> | <a class="reference external" href="https://antmicro.github.io/kenning/kenning-api.html">Core API</a> | <a class="reference external" href="https://opensource.antmicro.com/projects/kenning">kenning.ai</a></p>
<p><img alt="" src="_images/report-mosaic.png" /></p>
<p>Kenning aims towards providing modular execution blocks for:</p>
<ul class="simple">
<li><p>dataset management,</p></li>
<li><p>model training,</p></li>
<li><p>model optimization and compilation for a given target hardware,</p></li>
<li><p>running models using efficient runtimes on target device,</p></li>
<li><p>model evaluation and performance reports.</p></li>
</ul>
<p>These can be used seamlessly regardless of underlying frameworks for the above-mentioned steps.</p>
<p>Kenning’s aim is not to bring yet another training or compilation framework for deep learning models - there are lots of mature and versatile frameworks that support certain models, training routines, optimization techniques, hardware platforms and other components crucial to the deployment flow.
Still, there is no framework that would support all of the models or target hardware devices - especially the support matrix between compilation frameworks and target hardware is extremely sparse.
This means that any change in the application, especially in hardware, may end up in a necessity to change the entirety or a significant part of the application flow.</p>
<p>Kenning addresses this issue by providing a unified API that focuses on deployment tasks rather than their implementation - the developer decides which implementation should be used for each task, and with Kenning, it is possible to do in a seamless way.
This way, switching to another target platform results, in most cases, in a very small change in the code, instead of reimplementing larger parts of a project.
This is how Kenning can get the most out of the existing Deep Neural Network training and compilation frameworks.</p>
<p>Seamless nature of Kenning also allows developers to quickly evaluate the model on various stages of optimizations and compare them as shown in <a class="reference internal" href="#example-use-case-of-kenning">Example use case of Kenning</a>.</p>
<h2 id="kenning-installation">Kenning installation<a class="headerlink" href="#kenning-installation" title="Permalink to this heading">¶</a></h2>
<h3 id="module-installation-with-pip">Module installation with pip<a class="headerlink" href="#module-installation-with-pip" title="Permalink to this heading">¶</a></h3>
<p>To install Kenning with its basic dependencies with pip, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><code><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">antmicro</span><span class="o">/</span><span class="n">kenning</span><span class="o">.</span><span class="n">git</span>
</code></pre></div>
</div>
<p>Since Kenning can support various frameworks, and not all of them are required for users’ particular use cases, some of the requirements are optional.
We can distinguish the following groups of extra requirements:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> - modules for work with TensorFlow models (ONNX conversions, addons, and TensorFlow framework),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> - modules for work with PyTorch models,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mxnet</span></code> - modules for work with MXNet models,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nvidia_perf</span></code> - modules for performance measurements for NVIDIA GPUs,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">object_detection</span></code> - modules for work with YOLOv3 object detection and the Open Images Dataset V6 computer vision dataset,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">iree</span></code> - modules for IREE compilation and runtime,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tvm</span></code> - modules for Apache TVM compilation and runtime,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">onnxruntime</span></code> - modules for ONNX Runtime,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">docs</span></code> - modules for generating documentation,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test</span></code> - modules for Kenning framework testing,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">real_time_visualization</span></code> - modules for real time visualization runners.</p></li>
</ul>
<p>To install the extra requirements, e.g. <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code>, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><code><span class="n">sudo</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">antmicro</span><span class="o">/</span><span class="n">kenning</span><span class="o">.</span><span class="n">git</span><span class="c1">#egg=kenning[tensorflow]</span>
</code></pre></div>
</div>
<h3 id="working-directly-with-the-repository">Working directly with the repository<a class="headerlink" href="#working-directly-with-the-repository" title="Permalink to this heading">¶</a></h3>
<p>For development purposes, and for usage of additional resources (as sample scripts or trained models), clone repository with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><code><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">antmicro</span><span class="o">/</span><span class="n">kenning</span><span class="o">.</span><span class="n">git</span>
</code></pre></div>
</div>
<p>To download model weights, install <a class="reference external" href="https://git-lfs.com">Git Large File Storage</a> (if not installed) and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><code><span class="n">cd</span> <span class="n">kenning</span><span class="o">/</span>
<span class="n">git</span> <span class="n">lfs</span> <span class="n">pull</span>
</code></pre></div>
</div>
<h2 id="kenning-structure">Kenning structure<a class="headerlink" href="#kenning-structure" title="Permalink to this heading">¶</a></h2>
<p><img alt="" src="_images/class-flow.png" /></p>
<p>The <code class="docutils literal notranslate"><span class="pre">kenning</span></code> module consists of the following submodules:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">core</span></code> - provides interface APIs for datasets, models, optimizers, runtimes and runtime protocols,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">datasets</span></code> - provides implementations for datasets,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">modelwrappers</span></code> - provides implementations for models for various problems implemented in various frameworks,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compilers</span></code> - provides implementations for compilers and optimizers for deep learning models,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">runtimes</span></code> - provides implementations of runtime on target devices,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">interfaces</span></code> - provides interface classes to group related methods used in Kenning <code class="docutils literal notranslate"><span class="pre">core</span></code> classes,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">runtimeprotocols</span></code> - provides implementations for communication protocols between host and tested target,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dataproviders</span></code> - provides implementations for reading input data from various sources, such as camera, directories or TCP connections,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">outputcollectors</span></code> - provides implementations for processing outputs from models, i.e. saving results to file, or displaying predictions on screen.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">onnxconverters</span></code> - provides ONNX conversions for a given framework along with a list of models to test the conversion on,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">runners</span></code> - provide implementations for runners that can be used in runtime,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">report</span></code> - provides methods for rendering reports,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">drawing</span></code> - provides methods for rendering plots for reports,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resources</span></code> - contains project’s resources, like RST templates, or trained models,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scenarios</span></code> - contains executable scripts for running training, inference, benchmarks and other tests on target devices,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">utils</span></code> - various functions and classes used in all above-mentioned submodules,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tests</span></code> - submodules for framework testing.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">core</span></code> classes used throughout the entire Kenning framework:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class - performs dataset download, preparation, dataset-specific input preprocessing (i.e. input file opening, normalization), output postprocessing and model evaluation,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ModelWrapper</span></code> class - trains the model, prepares the model, performs model-specific input preprocessing and output postprocessing, runs inference on host using native framework,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> class - optimizes and compiles the model,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Runtime</span></code> class - loads the model, performs inference on compiled model, runs target-specific processing of inputs and outputs, and runs performance benchmarks,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RuntimeProtocol</span></code> class - implements the communication protocol between the host and the target,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DataProvider</span></code> class - implements data provision from such sources as camera, TCP connection or others for inference,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OutputCollector</span></code> class - implements parsing and utilization of data from inference (such as displaying the visualizations, sending the results to via TCP),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Runner</span></code> class - represents single runtime processing block.</p></li>
</ul>
<h2 id="kenning-usage">Kenning usage<a class="headerlink" href="#kenning-usage" title="Permalink to this heading">¶</a></h2>
<p>There are several ways to use Kenning:</p>
<ul class="simple">
<li><p>Using executable scripts from the <code class="docutils literal notranslate"><span class="pre">scenarios</span></code> submodule, configurable via JSON files (recommended approach);</p></li>
<li><p>Using executable scripts from the <code class="docutils literal notranslate"><span class="pre">scenarios</span></code> submodule, configurable via command-line arguments;</p></li>
<li><p>Using Kenning as a Python module.</p></li>
</ul>
<p>Kenning scenarios are executable scripts for:</p>
<ul class="simple">
<li><p>Model training and benchmarking using its native framework (<code class="docutils literal notranslate"><span class="pre">model_training</span></code>),</p></li>
<li><p>Model optimization and compilation for target hardware (<code class="docutils literal notranslate"><span class="pre">json_inference_tester</span></code> and <code class="docutils literal notranslate"><span class="pre">inference_tester</span></code>),</p></li>
<li><p>Model benchmarking on target hardware (<code class="docutils literal notranslate"><span class="pre">json_inference_tester</span></code>, <code class="docutils literal notranslate"><span class="pre">json_inference_server</span></code>, <code class="docutils literal notranslate"><span class="pre">inference_tester</span></code>, <code class="docutils literal notranslate"><span class="pre">inference_server</span></code>),</p></li>
<li><p>Rendering performance and quality reports from benchmark data (<code class="docutils literal notranslate"><span class="pre">render_report</span></code>).</p></li>
</ul>
<p>For more details on each of the above scenarios, check the <a class="reference external" href="https://antmicro.github.io/kenning/">Kenning documentation</a>.</p>
<h2 id="example-use-case-of-kenning">Example use case of Kenning<a class="headerlink" href="#example-use-case-of-kenning" title="Permalink to this heading">¶</a></h2>
<p>Let’s consider a simple scenario, where we want to optimize the inference time and memory usage of the classification model executed on a x86 CPU.</p>
<p>For this, we are going to use the <a class="reference external" href="https://github.com/antmicro/kenning/blob/main/kenning/datasets/pet_dataset.py"><code class="docutils literal notranslate"><span class="pre">PetDataset</span></code></a> Dataset and the <a class="reference external" href="https://github.com/antmicro/kenning/blob/main/kenning/modelwrappers/classification/tensorflow_pet_dataset.py"><code class="docutils literal notranslate"><span class="pre">TensorFlowPetDatasetMobileNetV2</span></code></a> ModelWrapper.</p>
<p>We will skip the training process. The trained model can be found in <code class="docutils literal notranslate"><span class="pre">kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5</span></code>.</p>
<p>The training of the above model can be performed using the following command:</p>
<!-- name="cmd-test" -->
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><code><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">kenning</span><span class="o">.</span><span class="n">scenarios</span><span class="o">.</span><span class="n">model_training</span> \
    <span class="n">kenning</span><span class="o">.</span><span class="n">modelwrappers</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">tensorflow_pet_dataset</span><span class="o">.</span><span class="n">TensorFlowPetDatasetMobileNetV2</span> \
    <span class="n">kenning</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">pet_dataset</span><span class="o">.</span><span class="n">PetDataset</span> \
    <span class="o">--</span><span class="n">logdir</span> <span class="n">build</span><span class="o">/</span><span class="n">logs</span> \
    <span class="o">--</span><span class="n">dataset</span><span class="o">-</span><span class="n">root</span> <span class="n">build</span><span class="o">/</span><span class="n">pet</span><span class="o">-</span><span class="n">dataset</span> \
    <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">path</span> <span class="n">build</span><span class="o">/</span><span class="n">trained</span><span class="o">-</span><span class="n">model</span><span class="o">.</span><span class="n">h5</span> \
    <span class="o">--</span><span class="n">batch</span><span class="o">-</span><span class="n">size</span> <span class="mi">32</span> \
    <span class="o">--</span><span class="n">learning</span><span class="o">-</span><span class="n">rate</span> <span class="mf">0.0001</span> \
    <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">epochs</span> <span class="mi">50</span>
</code></pre></div>
</div>
<h3 id="benchmarking-a-model-using-a-native-framework">Benchmarking a model using a native framework<a class="headerlink" href="#benchmarking-a-model-using-a-native-framework" title="Permalink to this heading">¶</a></h3>
<p>First of all, we want to check how the trained model performs using the native framework on CPU.</p>
<p>For this, we will use the <code class="docutils literal notranslate"><span class="pre">json_inference_tester</span></code> scenario.
Scenarios are configured using JSON format files.
In our case, the JSON file (named <code class="docutils literal notranslate"><span class="pre">native.json</span></code>) will look like this:</p>
<!-- name="json-scenario" -->
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_wrapper&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;dataset&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.datasets.pet_dataset.PetDataset&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;dataset_root&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/pet-dataset&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;download_dataset&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</div>
<p>This JSON provides a configuration for running the model natively and evaluating it against a defined Dataset.</p>
<p>For every class in the above JSON file, there are two keys required: <code class="docutils literal notranslate"><span class="pre">type</span></code> which is a module path of our class and <code class="docutils literal notranslate"><span class="pre">parameters</span></code> which is used to provide arguments used to create the instances of our classes.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">model_wrapper</span></code>, we specify the model used for evaluation - here it is MobileNetV2 trained on Pet Dataset. The <code class="docutils literal notranslate"><span class="pre">model_path</span></code> is the path to the saved model.
The <code class="docutils literal notranslate"><span class="pre">TensorFlowPetDatasetMobileNetV2</span></code> model wrapper provides methods for loading the model, preprocessing the inputs, postprocessing the outputs and running inference using the native framework (TensorFlow in this case).</p>
<p>The dataset provided for evaluation is Pet Dataset - here we specify that we want to download the dataset (<code class="docutils literal notranslate"><span class="pre">download_dataset</span></code>) to the <code class="docutils literal notranslate"><span class="pre">./build/pet-dataset</span></code> directory (<code class="docutils literal notranslate"><span class="pre">dataset_root</span></code>).
The <code class="docutils literal notranslate"><span class="pre">PetDataset</span></code> class can download the dataset (if necessary), load it, read the inputs and outputs from files, process them, and implement evaluation methods for the model.</p>
<p>With the above config saved in the <code class="docutils literal notranslate"><span class="pre">native.json</span></code> file, run the <code class="docutils literal notranslate"><span class="pre">json_inference_tester</span></code> scenario:</p>
<!-- name="json-scenario-runner" -->
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.json_inference_tester<span class="w"> </span>native.json<span class="w"> </span>build/native.json<span class="w"> </span>--verbosity<span class="w"> </span>INFO
</code></pre></div>
</div>
<p>This module runs inference based on the given configuration, evaluates the model and stores quality and performance metrics in JSON format, saved to the <code class="docutils literal notranslate"><span class="pre">build/native.json</span></code> file.
All below configurations can be executed with this command.</p>
<p>To visualize the evaluation and benchmark results, run the <code class="docutils literal notranslate"><span class="pre">render_report</span></code> module:</p>
<!-- name="json-scenario-runner" -->
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.render_report<span class="w"> </span><span class="s1">&#39;native&#39;</span><span class="w"> </span>build/benchmarks/native.md<span class="w"> </span>--measurements<span class="w"> </span>build/native.json<span class="w"> </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span>--img-dir<span class="w"> </span>build/benchmarks/imgs<span class="w"> </span>--verbosity<span class="w"> </span>INFO<span class="w"> </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification
</code></pre></div>
</div>
<p>This module takes the output JSON file generated by the <code class="docutils literal notranslate"><span class="pre">json_inference_tester</span></code> module, and creates a report titled <code class="docutils literal notranslate"><span class="pre">native</span></code>, which is saved in the <code class="docutils literal notranslate"><span class="pre">build/benchmarks/native.md</span></code> directory.
As specified in the <code class="docutils literal notranslate"><span class="pre">--report-types</span></code> flag, we create <code class="docutils literal notranslate"><span class="pre">peformance</span></code> and <code class="docutils literal notranslate"><span class="pre">classification</span></code> metrics sections in the report (for example, there is also a <code class="docutils literal notranslate"><span class="pre">detection</span></code> report type for object detection tasks).</p>
<p>In <code class="docutils literal notranslate"><span class="pre">build/benchmarks/imgs</span></code> there will be images with the <code class="docutils literal notranslate"><span class="pre">native_*</span></code> prefix visualizing the confusion matrix, CPU and memory usage, as well as inference time.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">build/benchmarks/native.md</span></code> file is a Markdown document containing a full report for the model - apart from linking to the generated visualizations, it provides aggregated information about CPU and memory usage, as well as classification quality metrics, such as accuracy, sensitivity, precision or G-Mean.
Such file can be included in a larger, Sphinx-based documentation, which allows easy, automated report generation, using e.g. CI, as can be seen in the <a class="reference external" href="https://antmicro.github.io/kenning/sample-report.html">Kenning documentation</a>.</p>
<p>While native frameworks are great for training and inference, model design, training on GPUs and distributing training across many devices, e.g. in a cloud environment, there is a fairly large variety of inference-focused frameworks for production purposes that focus on getting the most out of hardware in order to get results as fast as possible.</p>
<h3 id="optimizing-a-model-using-tensorflow-lite">Optimizing a model using TensorFlow Lite<a class="headerlink" href="#optimizing-a-model-using-tensorflow-lite" title="Permalink to this heading">¶</a></h3>
<p>One of such frameworks is TensorFlow Lite - a lightweight library for inferring networks on edge - it has a small binary size (which can be even more reduced by disabling unused operators) and a highly optimized format of input models, called FlatBuffers.</p>
<p>Before the TensorFlow Lite Interpreter (runtime for the TensorFlow Lite library) can be used, the model first needs to be optimized and compiled to the <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> format.</p>
<p>Let’s add a TensorFlow Lite Optimizer that will convert our MobileNetV2 model to a FlatBuffer format, as well as TensorFlow Lite Runtime that will execute the model:</p>
<!-- name="json-scenario" -->
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_wrapper&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;dataset&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.datasets.pet_dataset.PetDataset&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;dataset_root&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/pet-dataset&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;download_dataset&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.compilers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">            </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;default&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/fp32.tflite&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float32&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;float32&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;runtime&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.runtimes.tflite.TFLiteRuntime&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;save_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/fp32.tflite&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</div>
<p>In the already existing blocks, we only disable dataset download - the <code class="docutils literal notranslate"><span class="pre">download_dataset</span></code> parameter can be also removed, since the dataset is not downloaded by default.</p>
<p>The first new addition is the presence of the <code class="docutils literal notranslate"><span class="pre">optimizers</span></code> list - it allows us to add one or more objects inheriting from the <code class="docutils literal notranslate"><span class="pre">kenning.core.optimizer.Optimizer</span></code> class.
Optimizers read the model from the input file, apply various optimizations, and then save the optimized model to a new file.</p>
<p>In our current scenario, we will use the <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> class - it reads the model in a Keras-specific format, optimizes the model and saves it to the <code class="docutils literal notranslate"><span class="pre">./build/fp32.tflite</span></code> file.
The parameters of this particular Optimizer are worth noting here (each Optimizer usually has a different set of parameters):</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code> - indicates what the desired target device (or model type) is, <code class="docutils literal notranslate"><span class="pre">default</span></code> is the regular CPU. Another example here could be <code class="docutils literal notranslate"><span class="pre">edgetpu</span></code>, which can compile models for the Google Coral platform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compiled_model_path</span></code> - indicates where the model should be saved.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inference_input_type</span></code> and <code class="docutils literal notranslate"><span class="pre">inference_output_type</span></code> - indicate what the input and output type of the model should be. Usually, all trained models use FP32 weights (32-bit floating point) and activations - using <code class="docutils literal notranslate"><span class="pre">float32</span></code> here keeps the weights unchanged.</p></li>
</ul>
<p>The second thing that is added to the previous flow is the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> block - it provides a class inheriting from the <code class="docutils literal notranslate"><span class="pre">kenning.core.runtime.Runtime</span></code> class that is able to load the final model and run inference on target hardware. Usually, each <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> has a corresponding <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> able to run its results.</p>
<p>To compile the scenario (called <code class="docutils literal notranslate"><span class="pre">tflite-fp32.json</span></code>), run:</p>
<!-- name="json-scenario-runner" -->
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.json_inference_tester<span class="w"> </span>tflite-fp32.json<span class="w"> </span>build/tflite-fp32.json<span class="w"> </span>--verbosity<span class="w"> </span>INFO

python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.render_report<span class="w"> </span><span class="s1">&#39;tflite-fp32&#39;</span><span class="w"> </span>build/benchmarks/tflite-fp32.md<span class="w"> </span>--measurements<span class="w"> </span>build/tflite-fp32.json<span class="w"> </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span>--img-dir<span class="w"> </span>build/benchmarks/imgs<span class="w"> </span>--verbosity<span class="w"> </span>INFO<span class="w"> </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification
</code></pre></div>
</div>
<p>While it depends on the platform used, there should be a significant improvement in both inference time (model ca. 10-15x faster model compared to the native model) and memory usage (output model ca. 2x smaller).
What’s worth noting is that we get a significant improvement with no harm to the quality of the model - the outputs stay the same.</p>
<p><img alt="Confusion matrix" src="_images/confusion-matrix.png" /></p>
<h3 id="quantizing-a-model-using-tensorflow-lite">Quantizing a model using TensorFlow Lite<a class="headerlink" href="#quantizing-a-model-using-tensorflow-lite" title="Permalink to this heading">¶</a></h3>
<p>To further reduce memory usage, we can quantize the model - it is a process where all weights and activations in a model are calibrated to work with the <code class="docutils literal notranslate"><span class="pre">INT8</span></code> precision, instead of the <code class="docutils literal notranslate"><span class="pre">FP32</span></code> precision.
While it may severely harm the quality of the predictions, the quality reduction can be negligible with proper calibration.</p>
<p>The model can be quantized during the compilation process in TensorFlow Lite.
With Kenning, it can be achieved with the following simple additions:</p>
<!-- name="json-scenario" -->
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_wrapper&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;dataset&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.datasets.pet_dataset.PetDataset&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;dataset_root&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/pet-dataset&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.compilers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">            </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8.tflite&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;runtime&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.runtimes.tflite.TFLiteRuntime&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;save_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8.tflite&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</div>
<p>The only changes here in comparison to the previous configuration appear in the <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> configuration - we change <code class="docutils literal notranslate"><span class="pre">target</span></code>, <code class="docutils literal notranslate"><span class="pre">inference_input_type</span></code> and <code class="docutils literal notranslate"><span class="pre">inference_output_type</span></code> to <code class="docutils literal notranslate"><span class="pre">int8</span></code>.
Then, in the background, <code class="docutils literal notranslate"><span class="pre">TFLiteCompiler</span></code> fetches a subset of images from the <code class="docutils literal notranslate"><span class="pre">PetDataset</span></code> object to calibrate the model, and so the entire model calibration process happens automatically.</p>
<p>Let’s run the above scenario (<code class="docutils literal notranslate"><span class="pre">tflite-int8.json</span></code>):</p>
<!-- name="json-scenario-runner" -->
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.json_inference_tester<span class="w"> </span>tflite-int8.json<span class="w"> </span>build/tflite-int8.json<span class="w"> </span>--verbosity<span class="w"> </span>INFO

python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.render_report<span class="w"> </span><span class="s1">&#39;tflite-int8&#39;</span><span class="w"> </span>build/benchmarks/tflite-int8.md<span class="w"> </span>--measurements<span class="w"> </span>build/tflite-int8.json<span class="w"> </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span>--img-dir<span class="w"> </span>build/benchmarks/imgs<span class="w"> </span>--verbosity<span class="w"> </span>INFO<span class="w"> </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification
</code></pre></div>
</div>
<p>This results in a model over 7 times smaller compared to the native model without significant loss of accuracy, but without speed improvement.</p>
<h3 id="speeding-up-inference-with-apache-tvm">Speeding up inference with Apache TVM<a class="headerlink" href="#speeding-up-inference-with-apache-tvm" title="Permalink to this heading">¶</a></h3>
<p>To speed up inference of a quantized model, we can utilize vector extensions in x86 CPUs, more specifically AVX2.
For this, let’s use the Apache TVM framework to compile efficient runtimes for various hardware platforms.
The scenario looks like this:</p>
<!-- name="json-scenario" -->
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;model_wrapper&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.modelwrappers.classification.tensorflow_pet_dataset.TensorFlowPetDatasetMobileNetV2&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;dataset&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.datasets.pet_dataset.PetDataset&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;dataset_root&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/pet-dataset&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="nt">&quot;optimizers&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.compilers.tflite.TFLiteCompiler&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">            </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8.tflite&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_input_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;inference_output_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;int8&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.compilers.tvm.TVMCompiler&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="nt">&quot;target&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;llvm -mcpu=core-avx2&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;opt_level&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;conv2d_data_layout&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nt">&quot;compiled_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8_tvm.tar&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">],</span>
<span class="w">    </span><span class="nt">&quot;runtime&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;kenning.runtimes.tvm.TVMRuntime&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;parameters&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="nt">&quot;save_model_path&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;./build/int8_tvm.tar&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
</div>
<p>As it can be observed, addition of a new framework is just a matter of simply adding and configuring another optimizer and using the corresponding <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> to the final <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TVMCompiler</span></code>, with <code class="docutils literal notranslate"><span class="pre">llvm</span> <span class="pre">-mcpu=core-avx2</span></code> as the target, optimizes and compiles the model to use vector extensions. The final result is a <code class="docutils literal notranslate"><span class="pre">.tar</span></code> file containing a shared library that implements the entire model.</p>
<p>Let’s compile the scenario (<code class="docutils literal notranslate"><span class="pre">tvm-avx2-int8.json</span></code>):</p>
<!-- name="json-scenario-runner" -->
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.json_inference_tester<span class="w"> </span>tvm-avx2-int8.json<span class="w"> </span>build/tvm-avx2-int8.json<span class="w"> </span>--verbosity<span class="w"> </span>INFO

python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.render_report<span class="w"> </span><span class="s1">&#39;tvm-avx2-int8&#39;</span><span class="w"> </span>build/benchmarks/tvm-avx2-int8.md<span class="w"> </span>--measurements<span class="w"> </span>build/tvm-avx2-int8.json<span class="w"> </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span>--img-dir<span class="w"> </span>build/benchmarks/imgs<span class="w"> </span>--verbosity<span class="w"> </span>INFO<span class="w"> </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification
</code></pre></div>
</div>
<p>This results in a model over 40 times faster compared to the  native implementation, with size reduced 3x.</p>
<p>This shows how easily we can interconnect various frameworks and get the most out of the hardware using Kenning, while performing just minor alterations to the configuration file.</p>
<p>The summary of passes can be seen below:</p>
<table class="docutils data align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Speed boost</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Size reduction</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>native</p></td>
<td><p>1</p></td>
<td><p>0.9572730984</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>tflite-fp32</p></td>
<td><p>15.79405698</p></td>
<td><p>0.9572730984</p></td>
<td><p>1.965973551</p></td>
</tr>
<tr class="row-even"><td><p>tflite-int8</p></td>
<td><p>1.683232669</p></td>
<td><p>0.9519662539</p></td>
<td><p>7.02033412</p></td>
</tr>
<tr class="row-odd"><td><p>tvm-avx2-int8</p></td>
<td><p>41.61514549</p></td>
<td><p>0.9487005035</p></td>
<td><p>3.229375069</p></td>
</tr>
</tbody>
</table>
<h3 id="automated-model-comparison">Automated model comparison<a class="headerlink" href="#automated-model-comparison" title="Permalink to this heading">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">kenning.scenarios.render_report</span></code> script also allows us to compare evaluation results for multiple models.
Apart from creating a table with a summary of models, it also creates plots aggregating measurements collected during the evaluation process.</p>
<p>To create a comparison report for the above experiments, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><code>python<span class="w"> </span>-m<span class="w"> </span>kenning.scenarios.render_report<span class="w"> </span><span class="s2">&quot;summary-report&quot;</span>
<span class="w">    </span>build/benchmarks/summary.md<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--measurements<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/native.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tflite-fp32.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tflite-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>build/tvm-avx2-int8.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--root-dir<span class="w"> </span>build/benchmarks<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--report-types<span class="w"> </span>performance<span class="w"> </span>classification<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--img-dir<span class="w"> </span>build/benchmarks/imgs<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-names<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>native<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>tflite-fp32<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>tflite-int8<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>tvm-avx2-int8
</code></pre></div>
</div>
<p>Some examples of comparisons between various models rendered with the script:</p>
<ul>
<li><p>Accuracy, inference time and model size comparison:</p>
<p><img alt="" src="_images/accuracy-inference-time-comparison.png" /></p>
</li>
<li><p>Resource utilization distribution:</p>
<p><img alt="" src="_images/utilization-comparison.png" /></p>
</li>
<li><p>Comparison of classification metrics:</p>
<p><img alt="" src="_images/classification-metrics-comparison.png" /></p>
</li>
<li><p>And more</p></li>
</ul>
<h2 id="using-kenning-as-a-library-in-python-scripts">Using Kenning as a library in Python scripts<a class="headerlink" href="#using-kenning-as-a-library-in-python-scripts" title="Permalink to this heading">¶</a></h2>
<p>Kenning is also a regular Python module - after pip installation it can be used in Python scripts.
The example compilation of the model can look as follows:</p>
<!-- name="python-script" -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">kenning.datasets.pet_dataset</span> <span class="kn">import</span> <span class="n">PetDataset</span>
<span class="kn">from</span> <span class="nn">kenning.modelwrappers.classification.tensorflow_pet_dataset</span> <span class="kn">import</span> <span class="n">TensorFlowPetDatasetMobileNetV2</span>
<span class="kn">from</span> <span class="nn">kenning.compilers.tflite</span> <span class="kn">import</span> <span class="n">TFLiteCompiler</span>
<span class="kn">from</span> <span class="nn">kenning.runtimes.tflite</span> <span class="kn">import</span> <span class="n">TFLiteRuntime</span>
<span class="kn">from</span> <span class="nn">kenning.core.measurements</span> <span class="kn">import</span> <span class="n">MeasurementsCollector</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">PetDataset</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./build/pet-dataset/&#39;</span><span class="p">,</span>
    <span class="n">download_dataset</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TensorFlowPetDatasetMobileNetV2</span><span class="p">(</span>
    <span class="n">modelpath</span><span class="o">=</span><span class="s1">&#39;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&#39;</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_io_specification</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">modelpath</span><span class="p">)</span>
<span class="n">compiler</span> <span class="o">=</span> <span class="n">TFLiteCompiler</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">compiled_model_path</span><span class="o">=</span><span class="s1">&#39;./build/compiled-model.tflite&#39;</span><span class="p">,</span>
    <span class="n">modelframework</span><span class="o">=</span><span class="s1">&#39;keras&#39;</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;default&#39;</span><span class="p">,</span>
    <span class="n">inferenceinputtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">,</span>
    <span class="n">inferenceoutputtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span>
<span class="p">)</span>
<span class="n">compiler</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">inputmodelpath</span><span class="o">=</span><span class="s1">&#39;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&#39;</span>
<span class="p">)</span>
</code></pre></div>
</div>
<p>The above script downloads the dataset and compiles the model with FP32 inputs and outputs using TensorFlow Lite.</p>
<p>To get a quantized model, replace <code class="docutils literal notranslate"><span class="pre">target</span></code>, <code class="docutils literal notranslate"><span class="pre">inferenceinputtype</span></code> and <code class="docutils literal notranslate"><span class="pre">inferenceoutputtype</span></code> to <code class="docutils literal notranslate"><span class="pre">int8</span></code>:</p>
<!-- name="python-script" -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><code><span class="n">compiler</span> <span class="o">=</span> <span class="n">TFLiteCompiler</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
    <span class="n">compiled_model_path</span><span class="o">=</span><span class="s1">&#39;./build/compiled-model.tflite&#39;</span><span class="p">,</span>
    <span class="n">modelframework</span><span class="o">=</span><span class="s1">&#39;keras&#39;</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;int8&#39;</span><span class="p">,</span>
    <span class="n">inferenceinputtype</span><span class="o">=</span><span class="s1">&#39;int8&#39;</span><span class="p">,</span>
    <span class="n">inferenceoutputtype</span><span class="o">=</span><span class="s1">&#39;int8&#39;</span><span class="p">,</span>
    <span class="n">dataset_percentage</span><span class="o">=</span><span class="mf">0.3</span>
<span class="p">)</span>
<span class="n">compiler</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">inputmodelpath</span><span class="o">=</span><span class="s1">&#39;./kenning/resources/models/classification/tensorflow_pet_dataset_mobilenetv2.h5&#39;</span>
<span class="p">)</span>
</code></pre></div>
</div>
<p>To check how the compiled model is performing, create <code class="docutils literal notranslate"><span class="pre">TFLiteRuntime</span></code> object and run local model evaluation:</p>
<!-- name="python-script" -->
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><code><span class="n">runtime</span> <span class="o">=</span> <span class="n">TFLiteRuntime</span><span class="p">(</span>
    <span class="n">protocol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">modelpath</span><span class="o">=</span><span class="s1">&#39;./build/compiled-model.tflite&#39;</span>
<span class="p">)</span>

<span class="n">runtime</span><span class="o">.</span><span class="n">run_locally</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="s1">&#39;./build/compiled-model.tflite&#39;</span>
<span class="p">)</span>
<span class="n">MeasurementsCollector</span><span class="o">.</span><span class="n">save_measurements</span><span class="p">(</span><span class="s1">&#39;out.json&#39;</span><span class="p">)</span>
</code></pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">runtime.run_locally</span></code> method runs benchmarks of the model on the current device.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">MeasurementsCollector</span></code> class collects all benchmarks’ data for model inference and saves it in JSON format that can be later used to render reports with the <code class="docutils literal notranslate"><span class="pre">kenning.scenarios.render_report</span></code> script.</p>
<p>As it can be observed, all classes accessible from JSON files in these scenarios share their configuration a with the classes in the Python scripts mentioned above.</p>
<h2 id="adding-new-implementations">Adding new implementations<a class="headerlink" href="#adding-new-implementations" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, <code class="docutils literal notranslate"><span class="pre">ModelWrapper</span></code>, <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="docutils literal notranslate"><span class="pre">RuntimeProtocol</span></code>, <code class="docutils literal notranslate"><span class="pre">Runtime</span></code> and other classes from the <code class="docutils literal notranslate"><span class="pre">kenning.core</span></code> module have dedicated directories for their implementations.
Each method in the base classes that requires implementation raises an <code class="docutils literal notranslate"><span class="pre">NotImplementedError</span></code> exception.
They can be easily implemented or extended, but they need to conform to certain rules, usually described in the source documentation.</p>
<p>For more details and examples on how the Kenning framework can be adjusted and enhanced, follow the <a class="reference external" href="https://antmicro.github.io/kenning/">Kenning documentation</a>.
Implemented methods can be also overriden, if neccessary.</p>
<p>Most of the base classes implement <code class="docutils literal notranslate"><span class="pre">form_argparse</span></code> and <code class="docutils literal notranslate"><span class="pre">from_argparse</span></code> methods.
The former creates an argument parser and a group of arguments specific to the base class.
The latter creates an object of the class based on the arguments from argument parser.</p>
<p>Inheriting classes can modify <code class="docutils literal notranslate"><span class="pre">form_argparse</span></code> and <code class="docutils literal notranslate"><span class="pre">from_argparse</span></code> methods to provide better control over their processing, but they should always be based on the results of their base implementations.</p>


  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      2023-04-14
    
  </small>
</div>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="introduction.html" class="md-footer__link md-footer__link--prev" aria-label="Previous: Introduction" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Introduction
            </div>
          </div>
        </a>
      
      
        
        <a href="dl-deployment-stack.html" class="md-footer__link md-footer__link--next" aria-label="Next: Deep Learning deployment stack" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Deep Learning deployment stack
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  
  
  <div class="md-footer-meta md-typeset">
    
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-footer-copyright__highlight">
        Copyright &#169; 2020-2023, Antmicro.
        
    </div>
  
      <a href="https://github.com/antmicro/kenning/tree/1eeecb597a6ec543c04ad60eedee153c02323220">1eeecb59</a>
        @ <a href="https://github.com/antmicro/kenning/tree/main">main</a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/antmicro" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/antmicro" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
</div>
      
    </div>
    
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": ".", "features": ["toc.integrate"], "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
      
        <script src="_static/sphinx_immaterial_theme.cea6a0c1d5b0618cf.min.js"></script>
        <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    
  </body>
</html>